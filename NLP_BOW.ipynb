{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHa5fHRvvopxTQnUHtelYq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esashika/esashika_academy/blob/main/NLP_BOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Esashika Academy**\n",
        "\n",
        "**Autor:** Rhedson Esashika\n",
        "\n",
        "**Linkedin:** https://www.linkedin.com/in/esashika/"
      ],
      "metadata": {
        "id": "BVfTwyzpLBml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bag of Words (BoW)**\n",
        "\n",
        "O **Bag of Words** (traduzido livremente para “saco de palavras”) é um método simples e bastante utilizado no **Processamento de Linguagem Natural (PLN)** para representar textos de forma numérica, tornando-os compreensíveis para algoritmos de **Machine Learning**.\n",
        "\n",
        "> Em muitas fontes, você poderá ver a sigla “BoW” em inglês.\n",
        "\n"
      ],
      "metadata": {
        "id": "_80oxmq4uJH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pontos Fortes e Limitações**\n",
        "\n",
        "### Pontos Fortes\n",
        "\n",
        "- **Fácil de implementar**: é uma das técnicas mais simples para transformar texto em números.\n",
        "- **Rápido**: requer basicamente contar palavras e montar vetores.\n",
        "- **Serve de base para algoritmos de classificação** e outras tarefas de PLN, principalmente quando se tem muitos dados de texto e é importante ter um método simples para começar.\n",
        "\n",
        "### Limitações\n",
        "\n",
        "- **Não considera a ordem das palavras**: “O gato dorme na cama” e “Na cama, o gato dorme” geram a mesma representação, apesar de a ordem poder ser relevante em alguns casos.\n",
        "- **Não leva em conta a estrutura sintática** (sujeito, verbo, objeto, etc.).\n",
        "- **Gera vetores esparsos**: em grandes vocabulários, a maioria das posições pode ficar em zero.\n",
        "- **Palavras com significados diferentes** em contextos diversos são tratadas como iguais (por exemplo, “banco” de sentar e “banco” instituição financeira)."
      ],
      "metadata": {
        "id": "mg2DBE5AHBHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Implementando um BoW** \"*na mão*\"."
      ],
      "metadata": {
        "id": "MqsKfBxdrhTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1:** Preprocessing the Text Data"
      ],
      "metadata": {
        "id": "r-dDXffPpc7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download stopwords e tokenizer\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Sample text data: sentenças\n",
        "corpus = [\n",
        "    \"Python is amazing and fun.\",\n",
        "    \"Python is not just fun but also powerful.\",\n",
        "    \"Learning Python is fun!\",\n",
        "]\n",
        "\n",
        "# Função para pré-processar texto\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    # Tokenize: split the text into words\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Aplicar pré-processamento ao corpus de amostra\n",
        "processed_corpus = [preprocess(sentence) for sentence in corpus]\n",
        "print(processed_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITXjjC6yIY8P",
        "outputId": "5b6dec25-9a9a-4da4-e7b3-48fe0febb708"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['python', 'amazing', 'fun'], ['python', 'fun', 'also', 'powerful'], ['learning', 'python', 'fun']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2:** Build Vocabulary"
      ],
      "metadata": {
        "id": "IEaUC2S_pw--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialize um conjunto vazio para o vocabulário\n",
        "vocabulary = set()\n",
        "\n",
        "# Construindo o vocabulário\n",
        "for sentence in processed_corpus:\n",
        "    vocabulary.update(sentence)\n",
        "\n",
        "# Convertendo para uma lista ordenada\n",
        "vocabulary = sorted(list(vocabulary))\n",
        "print(\"Vocabulary:\", vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EGzz1yyp0DM",
        "outputId": "3b9623d4-f6ed-4a40-a7c7-c5ee743aad66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['also', 'amazing', 'fun', 'learning', 'powerful', 'python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3:** Calculate Word Frequencies and Vectorize"
      ],
      "metadata": {
        "id": "f38Yn2wqqBMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bow_vector(sentence, vocab):\n",
        "    vector = [0] * len(vocab)  # Inicializando o vetor com zeros\n",
        "    for word in sentence:\n",
        "        if word in vocab:\n",
        "            idx = vocab.index(word)  # Encontrando o índice da palavra no vocabulário\n",
        "            vector[idx] += 1\n",
        "    return vector\n",
        "\n",
        "\n",
        "# Criando um vetor BoW para cada frase no corpus processado\n",
        "bow_vectors = [create_bow_vector(sentence, vocabulary) for sentence in processed_corpus]\n",
        "print(\"Bag of Words Vectors:\")\n",
        "for vector in bow_vectors:\n",
        "    print(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELW51WSCqHr-",
        "outputId": "eda4d3aa-2068-40e4-e79d-d021cdcaef11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Vectors:\n",
            "[0, 1, 1, 0, 0, 1]\n",
            "[1, 0, 1, 0, 1, 1]\n",
            "[0, 0, 1, 1, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Usando o Scikit-learn’s CountVectorizer**"
      ],
      "metadata": {
        "id": "GyPQsVoesDIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Download das stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Lista de stopwords em inglês\n",
        "stop_words = list(stopwords.words(\"english\"))\n",
        "\n",
        "# Original corpus\n",
        "corpus = [\n",
        "    \"I loved the movie, it was fantastic!\",\n",
        "    \"The movie was okay, but not great.\",\n",
        "    \"I hated the movie, it was terrible.\",\n",
        "]\n",
        "\n",
        "# O objeto CountVectorizer será usado para transformar o texto em uma matriz de contagens de palavras,\n",
        "# removendo as stopwords do processamento.\n",
        "vectorizer = CountVectorizer(stop_words=stop_words)\n",
        "\n",
        "# O método .fit_transform() aprende o vocabulário do corpus e transforma os\n",
        "# textos em uma matriz esparsa (matriz que tem muitos elementos com valor zero,\n",
        "# ou seja, que é composta principalmente por zeros), onde cada linha representa\n",
        "# um documento e cada coluna representa uma palavra única do vocabulário.\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Exibir o vocabulário (palavras únicas extraídas do corpus)\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Converter a matriz esparsa em uma matriz comum para visualização\n",
        "print(\"BoW Representation:\")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klXuW-JuJSjB",
        "outputId": "d7820241-5166-4844-f10e-b879236cdf09"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['fantastic' 'great' 'hated' 'loved' 'movie' 'okay' 'terrible']\n",
            "BoW Representation:\n",
            "[[1 0 0 1 1 0 0]\n",
            " [0 1 0 0 1 1 0]\n",
            " [0 0 1 0 1 0 1]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Exemplo \"beeeem prático\" de utilização.**"
      ],
      "metadata": {
        "id": "XUiF6DcSNc6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Conjunto de frases\n",
        "corpus = [\n",
        "    \"I love this movie\", \"This film is amazing\", \"I really enjoyed the story\",\n",
        "    \"The acting was great\", \"What a fantastic experience\", \"I hate this movie\",\n",
        "    \"This film is terrible\", \"The story was boring\", \"I did not enjoy the acting\",\n",
        "    \"What a horrible experience\", \"I absolutely loved it\", \"This was the worst movie ever\",\n",
        "    \"I highly recommend this film\", \"I would not watch it again\", \"The performance was outstanding\",\n",
        "    \"The dialogues were poorly written\", \"It was an unforgettable movie\",\n",
        "    \"I disliked the character development\", \"One of the best films I have ever seen\",\n",
        "    \"The script was badly executed\", \"A masterpiece of storytelling\",\n",
        "    \"This film is not bad\", \"I never want to see this again\",\n",
        "    \"Incredible cinematography and acting\", \"I cannot stand this movie\",\n",
        "    \"The music and visuals were stunning\", \"The film was just okay\",\n",
        "    \"I think this was a very good movie\", \"The characters were well developed\",\n",
        "    \"This was a total disappointment\", \"I was very impressed with the film\",\n",
        "    \"The movie exceeded my expectations\", \"I wish I had never watched this\",\n",
        "    \"I wouldn't recommend it to anyone\", \"This was a well-crafted movie\",\n",
        "    \"I regret spending my time watching this\"\n",
        "]\n",
        "\n",
        "# Rótulos (1 = positivo, 0 = negativo)\n",
        "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
        "          0, 1, 1, 0, 1, 1, 0, 0, 1, 0]\n",
        "\n",
        "# Divisão dos dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Criaando um modelo Bag of Words melhorado\n",
        "# ngram_range=(1,2) → Considera palavras individuais (unigramas) e pares de palavras consecutivas (bigramas) para capturar melhor o contexto.\n",
        "# max_features=500 → Mantém apenas as 500 palavras/termos mais frequentes, reduzindo ruído e melhorando a eficiência do modelo.\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2), max_features=500)  # Inclui unigramas e bigramas, limita a 500 features\n",
        "\n",
        "# Transformar os textos em representações numéricas\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Criar e treinar o classificador Naive Bayes\n",
        "classifier = MultinomialNB(alpha=0.3)  # Melhor ajuste para suavização\n",
        "classifier.fit(X_train_bow, y_train)\n",
        "\n",
        "# Fazer previsões\n",
        "y_pred = classifier.predict(X_test_bow)\n",
        "\n",
        "# Avaliação do modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\" Precisão do modelo: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Testar com uma nova frase\n",
        "new_sentence = [\"I absolutely loved the story\"]\n",
        "new_sentence_bow = vectorizer.transform(new_sentence)\n",
        "prediction = classifier.predict(new_sentence_bow)\n",
        "\n",
        "# Exibir a predição\n",
        "print(f\" Sentimento previsto: {'Positivo' if prediction[0] == 1 else 'Negativo'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v83wz1iyNbS3",
        "outputId": "b3437060-c8c2-4acd-ff73-cb452ffb9a9f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Precisão do modelo: 62.50%\n",
            " Sentimento previsto: Positivo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BFZVXhH1uICs"
      }
    }
  ]
}